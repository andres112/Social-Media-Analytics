{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# Used to get multiple outputs per cell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main implementation\n",
    "#### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            User            Date  \\\n0     Judd Legum  11.03.20 13:25   \n1  CAPITÁN ADOBO  11.03.20 13:19   \n\n                                               Tweet  Binders  \\\n0  TRUMP TWO WEEKS AGO: \"You have 15 people [in t...      NaN   \n1  35 grados un 11 de marzo.\\n\\nLos sevillanos no...      NaN   \n\n                                           Permalink  Retweet count  \\\n0  https://www.twitter.com/user/status/1237731484...           5503   \n1  https://www.twitter.com/user/status/1237729790...           2117   \n\n   Likes count  Tweet value  \n0        19357       950.17  \n1         4978       100.28  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User</th>\n      <th>Date</th>\n      <th>Tweet</th>\n      <th>Binders</th>\n      <th>Permalink</th>\n      <th>Retweet count</th>\n      <th>Likes count</th>\n      <th>Tweet value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Judd Legum</td>\n      <td>11.03.20 13:25</td>\n      <td>TRUMP TWO WEEKS AGO: \"You have 15 people [in t...</td>\n      <td>NaN</td>\n      <td>https://www.twitter.com/user/status/1237731484...</td>\n      <td>5503</td>\n      <td>19357</td>\n      <td>950.17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CAPITÁN ADOBO</td>\n      <td>11.03.20 13:19</td>\n      <td>35 grados un 11 de marzo.\\n\\nLos sevillanos no...</td>\n      <td>NaN</td>\n      <td>https://www.twitter.com/user/status/1237729790...</td>\n      <td>2117</td>\n      <td>4978</td>\n      <td>100.28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "tweets_dataset = pd.read_csv('tweets_info_test.csv') \n",
    "user_dataset = pd.read_csv('user_info.csv') \n",
    "tweets_dataset.sort_values(by='Likes count', ascending = False)\n",
    "# user_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Tweets (Remove punctuation and stop-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 93
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1, 0, 2, 0, 0, 1, 1, 1, 1, 2, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n       [0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        1, 1, 1, 0, 0, 0, 0, 0, 1, 0]], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "# first create the transform and stop words set for english\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# The Tweet column is the list of documents\n",
    "vectorizer.fit(tweets_dataset['Tweet'])\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(tweets_dataset['Tweet'])\n",
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\n                input='content', lowercase=True, max_df=1.0, max_features=None,\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n                smooth_idf=True, stop_words='english', strip_accents=None,\n                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, use_idf=True, vocabulary=None)"
     },
     "metadata": {},
     "execution_count": 103
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('039', 0),\n ('11', 1),\n ('15', 2),\n ('35', 3),\n ('acabar', 4),\n ('ago', 5),\n ('cases', 6),\n ('close', 7),\n ('confirmed', 8),\n ('coronavirus', 9),\n ('couple', 10),\n ('days', 11),\n ('el', 12),\n ('going', 13),\n ('grados', 14),\n ('los', 15),\n ('marzo', 16),\n ('más', 17),\n ('nuestra', 18),\n ('para', 19),\n ('parte', 20),\n ('people', 21),\n ('podemos', 22),\n ('poner', 23),\n ('sevillanos', 24),\n ('states', 25),\n ('today', 26),\n ('trump', 27),\n ('united', 28),\n ('weeks', 29),\n ('ya', 30),\n ('zero', 31)]"
     },
     "metadata": {},
     "execution_count": 103
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1.40546511, 1.40546511, 1.40546511, 1.40546511, 1.40546511,\n       1.40546511, 1.40546511, 1.40546511, 1.40546511, 1.        ,\n       1.40546511, 1.40546511, 1.40546511, 1.40546511, 1.40546511,\n       1.40546511, 1.40546511, 1.40546511, 1.40546511, 1.40546511,\n       1.40546511, 1.40546511, 1.40546511, 1.40546511, 1.40546511,\n       1.40546511, 1.40546511, 1.40546511, 1.40546511, 1.40546511,\n       1.40546511, 1.40546511])"
     },
     "metadata": {},
     "execution_count": 103
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.21808824, 0.        , 0.43617648, 0.        , 0.        ,\n        0.21808824, 0.21808824, 0.21808824, 0.21808824, 0.31034316,\n        0.21808824, 0.21808824, 0.        , 0.21808824, 0.        ,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.        , 0.21808824, 0.        , 0.        , 0.        ,\n        0.21808824, 0.21808824, 0.21808824, 0.21808824, 0.21808824,\n        0.        , 0.21808824],\n       [0.        , 0.25394911, 0.        , 0.25394911, 0.25394911,\n        0.        , 0.        , 0.        , 0.        , 0.18068688,\n        0.        , 0.        , 0.25394911, 0.        , 0.25394911,\n        0.25394911, 0.25394911, 0.25394911, 0.25394911, 0.25394911,\n        0.25394911, 0.        , 0.25394911, 0.25394911, 0.25394911,\n        0.        , 0.        , 0.        , 0.        , 0.        ,\n        0.25394911, 0.        ]])"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(tweets_dataset['Tweet'])\n",
    "sorted(vectorizer.vocabulary_.items(), key = lambda k: k[1])\n",
    "vectorizer.idf_\n",
    "\n",
    "# encode document\n",
    "vectorizer.transform(tweets_dataset['Tweet']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}